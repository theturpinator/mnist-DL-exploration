{"cells":[{"cell_type":"markdown","source":["Note: this has much code that was highly inspired or directly taken from Chapter 4 of Fastbook, Fast.ai's machine learning book. In Ch 4, they create a binary MNIST classifier, distinguishing between 3's and 7's. This implementation is a multi-class classifier - a single model that can predict all 10 digits in the MNIST dataset."],"metadata":{"id":"F9IGuVJaoMth"}},{"cell_type":"markdown","metadata":{"id":"Ojh5vlZWy_oC"},"source":["# Setup"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3380,"status":"ok","timestamp":1726234800737,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"PB4UZq7XglaA"},"outputs":[],"source":["!pip install -Uqq fastai fastbook"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1726234800737,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"WKaOsDQehCMt"},"outputs":[],"source":["from fastai.vision.all import *\n","from fastbook import *\n","\n","matplotlib.rc('image', cmap='Greys') # set the default color palette to greys (0 is white, 255 is black)"]},{"cell_type":"markdown","metadata":{"id":"w9UCb-uZzHDD"},"source":["# Data Processing and Exploration"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1726234800737,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"f5bK815lsZJj","outputId":"015459a4-46af-4d88-c5c2-302ba3e12316"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#2) [Path('/root/.fastai/data/mnist_png/training'),Path('/root/.fastai/data/mnist_png/testing')]"]},"metadata":{},"execution_count":7}],"source":["# get MNIST dataset\n","path = untar_data(URLs.MNIST)\n","#Path.BASE_PATH = path\n","path.ls()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1726234800737,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"UBUWkBQCs8wY","outputId":"9555fcb6-a318-4620-9d00-be3acbe4457a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#10) [Path('/root/.fastai/data/mnist_png/training/0'),Path('/root/.fastai/data/mnist_png/training/1'),Path('/root/.fastai/data/mnist_png/training/2'),Path('/root/.fastai/data/mnist_png/training/3'),Path('/root/.fastai/data/mnist_png/training/4'),Path('/root/.fastai/data/mnist_png/training/5'),Path('/root/.fastai/data/mnist_png/training/6'),Path('/root/.fastai/data/mnist_png/training/7'),Path('/root/.fastai/data/mnist_png/training/8'),Path('/root/.fastai/data/mnist_png/training/9')]"]},"metadata":{},"execution_count":8}],"source":["# see all the folders (all numbers)\n","(path/'training').ls().sorted()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":45},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1726234800738,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"rX2GNQ3gutix","outputId":"8344fd35-22fe-410e-ecd8-8a767a30929f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABFElEQVR4nM2RIUgEURRFnzaDIAjiiBtn0LBpMEwS+5QVFBXDBsEtm7ZYhFWzWSwWQayuGAQREZNsEIMOaxCjE2TGHSbd6xrG/fNGrYI33fPP5/E+X+R/p9zi+1Ew/avbeAEIwP9hRhfOiUw+17654etMEADTtaJrk3xYEpl1lt8+Yk5qeUG8Vr/6Jh+xrlwt5Z1tqAmc5M5NEddzLAG3BkYC8lDN2SKb+VAAjpIlYEZEBkVExBbpdJS0RRYNxMCKXn0XmDLQY/FdbSaWAQJaTjCqiJZzap173qir+8Cl2wc3BKtK+gl5Opb11YAtV3TmIzDc8TzP2WN0MCTFVLrZd4Vpsm0OB/rFalg9/2n8WK7O5I/zCcqOjZOfpUxcAAAAAElFTkSuQmCC\n"},"metadata":{},"execution_count":9}],"source":["# get one image and view it\n","first_zero_path = (path/'training/0').ls()[0]\n","first_zero = Image.open(first_zero_path)\n","first_zero"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1726234800738,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"DrCKgEaevK2R","outputId":"0cf413e7-c003-4cb1-8887-e5eb32d3cc30"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  45, 178, 253, 242, 166, 216,  41,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 120, 225, 252, 252, 253, 252, 252, 252,  76,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,  85, 185, 253, 252, 252, 252, 253, 252, 252, 252, 223, 104,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  13, 194, 252, 252, 253, 252, 252, 252, 253, 252, 252, 252, 253, 246, 100,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,  13, 204, 253, 253, 253, 214,  88,   0,   0,  63,  38,  89, 238, 254, 241, 253,  28,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0, 188, 253, 252, 233,  96,   0,   0,   0,   0,   0,   0,   0, 125, 253, 215, 252, 103,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0, 104, 246, 253, 208,  37,   0,   0,   0,   0,   0,   0,   0,   0,   0, 128, 252, 252, 177,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,  51, 246, 252, 241, 109,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  29, 252, 252, 202,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,  16, 216, 253, 253, 163,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 129, 253, 253, 128,   0,   0,   0],\n","        [  0,   0,   0,   0,   0, 104, 252, 252, 252,  38,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  29, 252, 252,  53,   0,   0,   0],\n","        [  0,   0,   0,   0,   0, 141, 252, 252, 214,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  66, 252, 252, 139,   0,   0,   0],\n","        [  0,   0,   0,   0,   0, 241, 252, 252,  90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 141, 252, 252,  40,   0,   0,   0],\n","        [  0,   0,   0,   0,   0, 255, 253, 253,  28,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 204, 253, 244,  25,   0,   0,   0],\n","        [  0,   0,   0,   0,   0, 253, 252, 252,  28,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  26, 253, 240,  81,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0, 253, 252, 252,  65,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  29, 210, 253, 196,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0, 153, 252, 252, 190,  51,   0,   0,   0,   0,   0,   0,   0,   0,  51, 234, 252, 253,  96,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,  76, 244, 253, 253, 179,  22,   0,   0,   0,   0,   0,   0,  92, 216, 253, 178,  51,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,  82, 240, 252, 253, 234, 131,  57,  57,  57,  38, 151, 253, 240, 158,   9,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,  81, 243, 253, 252, 252, 252, 253, 252, 234, 246, 244, 130,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,  25, 140, 165, 164, 240, 203, 227, 139, 139,  75,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=torch.uint8)"]},"metadata":{},"execution_count":10}],"source":["# convert it to tensor\n","tensor(first_zero)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":16500,"status":"ok","timestamp":1726234817233,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"jQlK-0_lvrfh"},"outputs":[],"source":["train_images = []\n","valid_images = []\n","\n","# convert all training and test data to tensors\n","for i in range(10):\n","  paths = (path/f'training/{i}').ls()\n","  train_images.append(torch.stack([tensor(Image.open(x)) for x in paths]).float()/255) #create image tensor and scale pixel values between 0 and 1\n","\n","  paths = (path/f'testing/{i}').ls()\n","  valid_images.append(torch.stack([tensor(Image.open(x)) for x in paths]).float()/255)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1726234817233,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"5qvj9zQfxGqm","outputId":"f2826879-8b67-43ec-867f-391c8559d693"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Axes: >"]},"metadata":{},"execution_count":12},{"output_type":"display_data","data":{"text/plain":["<Figure size 100x100 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# sample one of the 4s\n","show_image(train_images[4][20])"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1726234817233,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"-o_s8GG6yX_3","outputId":"0d34f62c-d4ac-4d27-d669-9a0dff28f508"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5923, 28, 28])"]},"metadata":{},"execution_count":13}],"source":["# check out the shape of one of the 3d tensors (rank 3)\n","train_images[0].shape\n","\n","# notice there are 5923 training instances (images), each 28 pixels by 28 pixels"]},{"cell_type":"markdown","metadata":{"id":"WGUPvUpwaFSs"},"source":["# Calculating Loss"]},{"cell_type":"markdown","metadata":{"id":"Pi2-Nvggzfos"},"source":["**Getting the \"ideal\" digit for each number (average)**\n","\n","Couldn't we simply get the pixel-wise average of each number's training set, and then see how far away our test data is from that average by getting the average delta of each pixel? We could calculate the L1 or L2 norm (MAE or RMSE) of that instance from each averaged digit - the lowest error result would be our prediction.\n","\n","Let's continue with this as our evaluation method for now, until we introduce a better method (gradient descent) later."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1726234817234,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"pNQ5_jxCysCi","outputId":"71157f29-a0d0-4f8a-e866-d61898f5270e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Axes: >"]},"metadata":{},"execution_count":14},{"output_type":"display_data","data":{"text/plain":["<Figure size 100x100 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Get the pixel-wise average of the 0s to get the \"ideal\" 0\n","ideal_zero = train_images[0].mean(0)\n","show_image(ideal_zero)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1726234817234,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"b-qOi4160AqK","outputId":"51532eb5-ec59-438e-ddff-fec7d22beb02"},"outputs":[{"output_type":"stream","name":"stdout","text":["MAE: 0.14024855196475983\n","RMSE: 0.37449774146080017\n"]},{"output_type":"execute_result","data":{"text/plain":["<Axes: >"]},"metadata":{},"execution_count":15},{"output_type":"display_data","data":{"text/plain":["<Figure size 100x100 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 100x100 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 100x100 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Get the difference from one of our testing instances, just as an example\n","delta_tensor = (valid_images[0][20] - ideal_zero).abs()\n","\n","# calculate the mean absolute error (MAE - just the average of our delta tensor)\n","mae = delta_tensor.mean()\n","print(f'MAE: {mae}')\n","\n","# we can also determine the error with root mean squared error (RMSE), where\n","# larger differences are penalized more\n","squared_error_delta_tensor = ((valid_images[0][20] - ideal_zero)**2)\n","rmse = delta_tensor.mean().sqrt()\n","print(f'RMSE: {rmse}')\n","\n","#Random image of 0 from testing set\n","show_image(valid_images[0][20])\n","\n","#ideal 0\n","show_image(ideal_zero)\n","\n","#delta tensor\n","show_image(delta_tensor)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1726234817234,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"xSwehbdAZ8x5","outputId":"07eb9f8d-caa8-40c2-f0c6-30acb1c8ec01"},"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor(0.1436), tensor(0.0664), tensor(0.1453), tensor(0.1296), tensor(0.1197), tensor(0.1378), tensor(0.1273), tensor(0.1088), tensor(0.1326), tensor(0.1118)]\n","[tensor(0.2508), tensor(0.1688), tensor(0.2529), tensor(0.2364), tensor(0.2279), tensor(0.2462), tensor(0.2383), tensor(0.2170), tensor(0.2407), tensor(0.2194)]\n"]}],"source":["# Let's use this method and see how accurate the predictions across our whole test set are\n","\n","def create_ideal_nums(X):\n","  # get all ideal nums\n","\n","  ideal_nums = []\n","  for i in range(10):\n","    ideal_nums.append(X[i].mean(0)) # perform pixel-wise average across all training instances\n","\n","  return ideal_nums\n","\n","\n","def calculate_class_mae(X, ideal_nums):\n","  mean_absolute_errors = []\n","\n","  for i in range(len(X)):\n","    mean_absolute_errors.append((X[i] - ideal_nums[i]).abs().mean()) #broadcast ideal num and get difference between all test instances, take absolute value, and get average\n","\n","  return mean_absolute_errors\n","\n","\n","def calculate_class_rmse(X, ideal_nums):\n","  root_mean_squared_errors = []\n","\n","  for i in range(len(X)):\n","    root_mean_squared_errors.append(((X[i] - ideal_nums[i])**2).mean().sqrt()) #similar, but do RMSE instead\n","\n","  return root_mean_squared_errors\n","\n","\n","ideal_nums = create_ideal_nums(train_images)\n","print(calculate_class_mae(valid_images, ideal_nums))\n","print(calculate_class_rmse(valid_images, ideal_nums))"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1726234817234,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"rUMBSMU7m8d4","outputId":"d1109d7f-79a3-4ec6-bb45-4cae724a0e68"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.1436)\n","tensor(0.2508)\n"]}],"source":["# Use the built-in Pytorch loss functions for l1 and l2 loss to ensure our manual calculation was accurate (checking with 0 class only)\n","ideal_zero_tensor = torch.stack([ideal_nums[0] for i in range(len(valid_images[0]))]) # makes one copy of the ideal zero for\n","print(F.l1_loss(valid_images[0].float(),ideal_zero_tensor))\n","print(F.mse_loss(valid_images[0].float(),ideal_zero_tensor).sqrt())"]},{"cell_type":"markdown","metadata":{"id":"V66S4cvqoYsj"},"source":["# Predictions\n","\n","Up to now, we've learned how to calculate the overall loss using the method described above. However, we want to use these losses to make a prediction and then measure the accuracy of said prediction."]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1726234817234,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"Y3B1IVXFoqxc"},"outputs":[],"source":["# Let's get the MAE of each test instance from each ideal image. The image with the lowest MAE will be what we consider the prediction.\n","\n","def get_predictions():\n","  predictions = []\n","\n","  for i in range(10):\n","    mae_vals = []\n","\n","    # take training instances and compare each to all 10 ideal digits. Get the MAE for each training instance.\n","    for j in range(10):\n","      mae_vals.append((valid_images[i] - ideal_nums[j]).abs().mean((-1,-2))) # get the absolute difference of each image vs. the ideal, and then get the average across each delta\n","\n","    mae_tensor = torch.stack(mae_vals) # stack the arrays of instance-level means against each ideal number into a tensor\n","    predictions.append(mae_tensor.argmin(0)) # For each training instance, get the index of the minimum loss. That is our prediction (no need to map index to value since our labels are 0-9)\n","\n","  return predictions\n","\n","preds = get_predictions()\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":339,"status":"ok","timestamp":1726234817568,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"8WuvojityBlG","outputId":"5431f76d-a928-4ecd-8d8a-54e7b14b4585"},"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor(0.8153), tensor(0.9982), tensor(0.4234), tensor(0.6089), tensor(0.6680), tensor(0.3262), tensor(0.7871), tensor(0.7646), tensor(0.4425), tensor(0.7760)]\n","tensor(0.6685)\n"]}],"source":["def get_accuracy():\n","  accuracy = []\n","  num_correct = 0\n","  num_total = 0\n","\n","  # calculate accuracy\n","  for i in range(10):\n","    accuracy.append((preds[i] == i).sum()/len(preds[i]))\n","    num_correct += (preds[i] == i).sum()\n","    num_total += len(preds[i])\n","\n","  return accuracy, num_correct/num_total\n","\n","digit_acc, overall_acc = get_accuracy()\n","print(digit_acc) # digit-wise accuracy\n","print(overall_acc) #overall accuracy (~67%)\n","\n","#note some predictions do really well! For example, 1's have a very high accuracy\n","#however, some have horrible accuracy, like 2's as 42%"]},{"cell_type":"markdown","metadata":{"id":"flIlPnKTaX0q"},"source":["# Using a Neural Network with SGD"]},{"cell_type":"markdown","metadata":{"id":"sXXzyRFVaioj"},"source":["Prepare Data"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1726234817568,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"ZL2s-1rSaf6h","outputId":"618e23b0-7168-4809-f471-f47afba71a08"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([60000, 784]) torch.Size([60000]) torch.Size([10000, 784]) torch.Size([10000])\n","tensor([0, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9])\n"]}],"source":["# Create a tabular training (and validation) set of our independent variables (i.e., our pixels )\n","# - note, we must flatten our images into 1D pixel lists.\n","\n","def create_dataset(data):\n","  X = []\n","  y = []\n","\n","  for i in range(10): # for each category (digit)...\n","    test = data[i].view(-1, 28*28)\n","    X.append(data[i].view(-1, 28*28)) #flatten each image into a 1D array\n","    y += [i]*len(data[i])\n","\n","  return torch.concat(X), tensor(y)\n","\n","train_X, train_y = create_dataset(train_images)\n","valid_X, valid_y = create_dataset(valid_images)\n","\n","print(train_X.shape, train_y.shape, valid_X.shape, valid_y.shape)\n","print(train_y[::5000]) # print some of the labels from the training set"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1023,"status":"ok","timestamp":1726234818590,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"JBnXzd01a2Tg","outputId":"fd40d81a-420b-4bf4-da14-e94a62f573e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["784 tensor(0)\n"]}],"source":["# zip the X and y's together for our final datasets\n","training_dataset = list(zip(train_X, train_y))\n","test_dataset = list(zip(valid_X, valid_y))\n","\n","print(len(training_dataset[0][0]), training_dataset[0][1]) #validate that we have a set of features (all 784 pixels), followed by the label"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1726234818590,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"Ng476USmpPQL"},"outputs":[],"source":["#convert our datasets into DataLoader objects, creating shuffled mini-batches\n","training_dl = DataLoader(training_dataset, batch_size=256)\n","valid_dl = DataLoader(test_dataset, batch_size=256)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1726234818590,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"NTfJmnqWa8UV"},"outputs":[],"source":["# randomly initialize our parameters (i.e., the weights for each pixel)\n","def init_params(size, std=1):\n","  return (torch.randn(size)*std).requires_grad_() # create a tensor filled with values near-zero that follow a normal distribution\n","\n","\n","w1 = init_params((28*28, 30)) # create one param for each connection from pixel to each of the 30 nodes in the first hidden layer\n","b1 = init_params(30) # create a bias parameter for each node in the hidden layer\n","w2 = init_params((30, 10)) # create a param for each output from the hidden layer to the 10 output nodes\n","b2 = init_params(10) # create one bias param for each of the 10 output nodes"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1726234818590,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"AWVQ648k3QR6"},"outputs":[],"source":["def simple_net(X):\n","    res = X@w1 + b1\n","    res = res.max(tensor(0.0)) #ReLU to add non-linearity\n","    res = res@w2 + b2\n","    return res"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1726234818590,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"CmqkQd5hRSDp"},"outputs":[],"source":["# create a loss function. We will use Cross Entropy Loss as opposed to MSE, since\n","# MSE is suitable for regression model, but not for classification.\n","ce_loss = nn.CrossEntropyLoss()\n","\n","# create a function that calculates the gradient\n","def calc_grad(X, y, model):\n","    preds = model(X)\n","    loss = ce_loss(preds, y)\n","    loss.backward()\n","\n","def train_epoch(model, lr, params):\n","    for X, y in training_dl: # for each batch...\n","        calc_grad(train_X, train_y, model)\n","        for p in params:\n","            p.data -= p.grad*lr # step each param layer\n","            p.grad.zero_() # set gradient for these params back to zero\n"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1726234818590,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"MOj5SxXDbcTX"},"outputs":[],"source":["def batch_accuracy(X, y):\n","  preds = (torch.argmax(X, axis=1), y)\n","  correct = preds[0] == preds[1]\n","  return correct.float().mean() # calculates the overall accuracy for the batch\n","\n","def validate_epoch(model):\n","  accs = [batch_accuracy(model(X), y) for X,y in valid_dl]\n","  return round(torch.stack(accs).mean().item(), 4) # get the mean accuracy"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11717,"status":"ok","timestamp":1726234830305,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"qn8mLYS-sfkR","outputId":"19488646-c3a4-4d19-eb71-a55bc50aba07"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.6586\n"]}],"source":["# train for 1 epoch\n","lr = .1\n","params = w1,b1,w2,b2\n","\n","train_epoch(simple_net, lr, params)\n","\n","print(validate_epoch(simple_net)) # ~66%"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27755,"status":"ok","timestamp":1726234858058,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"lXBCB8f9uKMo","outputId":"c9983866-926a-48c9-ce4d-f912aad5e9d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.7755\n"]}],"source":["# train a few more epochs and see if we improve\n","train_epoch(simple_net, lr, params)\n","train_epoch(simple_net, lr, params)\n","train_epoch(simple_net, lr, params)\n","\n","print(validate_epoch(simple_net)) # ~78%"]},{"cell_type":"markdown","metadata":{"id":"5nIALaNSutXb"},"source":["Now, using PyTorch's built-in classes/methods"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1726234858058,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"aXDwGW0uuyVO"},"outputs":[],"source":["# define model architecture (same as before)\n","# - 784 inputs\n","# - 30 node hidden layer w/ ReLU activation (for non-linearity)\n","# - 10 node output layer\n","model = nn.Sequential(\n","    nn.Linear(28*28,30),\n","    nn.ReLU(),\n","    nn.Linear(30,10)\n",")"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1726234858059,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"CLrpq2Juv39i"},"outputs":[],"source":["# make an optimizer\n","class BasicOptim:\n","    def __init__(self,params,lr):\n","      self.params = list(params)\n","      self.lr = lr\n","\n","    def step(self, *args, **kwargs):\n","        for p in self.params:\n","          p.data -= p.grad.data * self.lr\n","\n","    def zero_grad(self, *args, **kwargs):\n","        for p in self.params:\n","          p.grad = None"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1726234858059,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"vEP9SXm5w_Ev"},"outputs":[],"source":["opt = BasicOptim(model.parameters(), lr)"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1726234858059,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"BnVEP30dxPJG"},"outputs":[],"source":["def train_epoch(model):\n","    for X,y in training_dl:\n","        calc_grad(X, y, model)\n","        opt.step()\n","        opt.zero_grad()"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1726234858059,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"gP01bv_Axmfk","outputId":"412fb678-464e-4886-b2cb-74b0b3956bac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0688"]},"metadata":{},"execution_count":33}],"source":["validate_epoch(model) #default accuracy is bad!"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1726234858059,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"6OKCoZSGxrYk"},"outputs":[],"source":["def train_model(model, epochs):\n","    for i in range(epochs):\n","        train_epoch(model)\n","        if i%10==0:\n","          print(validate_epoch(model), end=' ')"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26727,"status":"ok","timestamp":1726234884782,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"a_BT3p38xvbm","outputId":"263c3515-9854-4009-8b23-d86b952b656f"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.122 0.3048 0.4252 0.4391 0.5612 0.584 "]}],"source":["lr = .01\n","train_model(model, 60) # after 60 epochs, our accuracy is ~58%"]},{"cell_type":"markdown","source":["Now let's simplify things even further by exclusively using Fast.ai and Pytorch's built-in classes for our optimizer and training"],"metadata":{"id":"2afl39PIZV_d"}},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":365,"status":"ok","timestamp":1726234896312,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"e87EPpcDyY1K"},"outputs":[],"source":["#let's make our hidden layer larger and see if our accuracy improves\n","# - 784 inputs (each pixel)\n","# - 200 node hidden layer w/ ReLU activation (for non-linearity)\n","# - 10 node output layer\n","model = nn.Sequential(\n","    nn.Linear(28*28,200),\n","    nn.ReLU(),\n","    nn.Linear(200,10)\n",")\n","\n","#bigger batches\n","training_dl = DataLoader(training_dataset, batch_size=512)\n","valid_dl = DataLoader(test_dataset, batch_size=512)\n","\n","lr = .01\n","dls = DataLoaders(training_dl, valid_dl)\n","ce_loss = nn.CrossEntropyLoss()\n","learn = Learner(dls, model, opt_func=SGD,\n","                loss_func=ce_loss, metrics=batch_accuracy)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":65117,"status":"ok","timestamp":1726234964539,"user":{"displayName":"Brendan Turpin","userId":"02582185490460692771"},"user_tz":240},"id":"7qlAKtZQy9tJ","outputId":"ad16017c-39d4-40e4-ffc4-b7c07eacf6a8"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>batch_accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>2.225323</td>\n","      <td>2.127984</td>\n","      <td>0.256400</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>1.887695</td>\n","      <td>1.991606</td>\n","      <td>0.150500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.493284</td>\n","      <td>1.901361</td>\n","      <td>0.212400</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.172991</td>\n","      <td>1.735255</td>\n","      <td>0.323700</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.956178</td>\n","      <td>1.568925</td>\n","      <td>0.384800</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.816242</td>\n","      <td>1.435222</td>\n","      <td>0.423400</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.723018</td>\n","      <td>1.328854</td>\n","      <td>0.456800</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.657468</td>\n","      <td>1.241340</td>\n","      <td>0.484600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.608985</td>\n","      <td>1.167170</td>\n","      <td>0.512600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.571588</td>\n","      <td>1.103180</td>\n","      <td>0.538200</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.541774</td>\n","      <td>1.047189</td>\n","      <td>0.563000</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.517354</td>\n","      <td>0.997953</td>\n","      <td>0.586500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.496953</td>\n","      <td>0.954225</td>\n","      <td>0.607900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.479657</td>\n","      <td>0.915163</td>\n","      <td>0.626100</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.464782</td>\n","      <td>0.880060</td>\n","      <td>0.642900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.451813</td>\n","      <td>0.848376</td>\n","      <td>0.658600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.440390</td>\n","      <td>0.819720</td>\n","      <td>0.673800</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.430240</td>\n","      <td>0.793681</td>\n","      <td>0.685500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.421172</td>\n","      <td>0.769844</td>\n","      <td>0.697300</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.412986</td>\n","      <td>0.748028</td>\n","      <td>0.706600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.405559</td>\n","      <td>0.728001</td>\n","      <td>0.715200</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.398784</td>\n","      <td>0.709598</td>\n","      <td>0.722600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.392566</td>\n","      <td>0.692528</td>\n","      <td>0.731100</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.386849</td>\n","      <td>0.676672</td>\n","      <td>0.739800</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.381553</td>\n","      <td>0.661856</td>\n","      <td>0.746600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.376598</td>\n","      <td>0.648096</td>\n","      <td>0.754000</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.371985</td>\n","      <td>0.635191</td>\n","      <td>0.760400</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.367657</td>\n","      <td>0.623133</td>\n","      <td>0.767800</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.363600</td>\n","      <td>0.611787</td>\n","      <td>0.772900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.359770</td>\n","      <td>0.601102</td>\n","      <td>0.778400</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.356153</td>\n","      <td>0.590994</td>\n","      <td>0.783000</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.352715</td>\n","      <td>0.581454</td>\n","      <td>0.787900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.349440</td>\n","      <td>0.572421</td>\n","      <td>0.792500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.346327</td>\n","      <td>0.563813</td>\n","      <td>0.795800</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.343336</td>\n","      <td>0.555614</td>\n","      <td>0.799600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.340475</td>\n","      <td>0.547811</td>\n","      <td>0.803000</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.337715</td>\n","      <td>0.540382</td>\n","      <td>0.805800</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.335063</td>\n","      <td>0.533231</td>\n","      <td>0.808800</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.332516</td>\n","      <td>0.526363</td>\n","      <td>0.812500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.330059</td>\n","      <td>0.519739</td>\n","      <td>0.815400</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.327667</td>\n","      <td>0.513424</td>\n","      <td>0.819300</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.325355</td>\n","      <td>0.507351</td>\n","      <td>0.821900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.323102</td>\n","      <td>0.501470</td>\n","      <td>0.824700</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.320928</td>\n","      <td>0.495811</td>\n","      <td>0.827200</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.318813</td>\n","      <td>0.490329</td>\n","      <td>0.828600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.316741</td>\n","      <td>0.485004</td>\n","      <td>0.830900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.314723</td>\n","      <td>0.479855</td>\n","      <td>0.832900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.312754</td>\n","      <td>0.474868</td>\n","      <td>0.835200</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.310822</td>\n","      <td>0.470016</td>\n","      <td>0.836800</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.308923</td>\n","      <td>0.465345</td>\n","      <td>0.839000</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.307074</td>\n","      <td>0.460774</td>\n","      <td>0.840800</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.305251</td>\n","      <td>0.456338</td>\n","      <td>0.842600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.303486</td>\n","      <td>0.451978</td>\n","      <td>0.844600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.301744</td>\n","      <td>0.447715</td>\n","      <td>0.846300</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.300029</td>\n","      <td>0.443571</td>\n","      <td>0.848500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.298340</td>\n","      <td>0.439544</td>\n","      <td>0.850400</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.296685</td>\n","      <td>0.435591</td>\n","      <td>0.851100</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.295063</td>\n","      <td>0.431716</td>\n","      <td>0.852900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.293463</td>\n","      <td>0.427932</td>\n","      <td>0.855100</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.291879</td>\n","      <td>0.424228</td>\n","      <td>0.856000</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.290319</td>\n","      <td>0.420556</td>\n","      <td>0.857300</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.288780</td>\n","      <td>0.416975</td>\n","      <td>0.858900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.287258</td>\n","      <td>0.413485</td>\n","      <td>0.860600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.285744</td>\n","      <td>0.410054</td>\n","      <td>0.861700</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.284261</td>\n","      <td>0.406669</td>\n","      <td>0.863500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.282793</td>\n","      <td>0.403336</td>\n","      <td>0.864900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.281343</td>\n","      <td>0.400059</td>\n","      <td>0.866900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.279911</td>\n","      <td>0.396829</td>\n","      <td>0.868400</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.278494</td>\n","      <td>0.393670</td>\n","      <td>0.869600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.277088</td>\n","      <td>0.390568</td>\n","      <td>0.871400</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.275683</td>\n","      <td>0.387502</td>\n","      <td>0.872600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.274301</td>\n","      <td>0.384506</td>\n","      <td>0.874600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.272932</td>\n","      <td>0.381567</td>\n","      <td>0.876100</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.271561</td>\n","      <td>0.378666</td>\n","      <td>0.877600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.270207</td>\n","      <td>0.375776</td>\n","      <td>0.878400</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.268876</td>\n","      <td>0.372974</td>\n","      <td>0.879500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.267546</td>\n","      <td>0.370248</td>\n","      <td>0.880600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.266218</td>\n","      <td>0.367522</td>\n","      <td>0.881700</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.264914</td>\n","      <td>0.364851</td>\n","      <td>0.882700</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.263619</td>\n","      <td>0.362234</td>\n","      <td>0.883500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.262319</td>\n","      <td>0.359671</td>\n","      <td>0.884500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.261036</td>\n","      <td>0.357148</td>\n","      <td>0.885200</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.259771</td>\n","      <td>0.354660</td>\n","      <td>0.886200</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.258514</td>\n","      <td>0.352215</td>\n","      <td>0.887000</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.257265</td>\n","      <td>0.349788</td>\n","      <td>0.887900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.256012</td>\n","      <td>0.347403</td>\n","      <td>0.888400</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.254785</td>\n","      <td>0.345077</td>\n","      <td>0.889500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.253574</td>\n","      <td>0.342745</td>\n","      <td>0.890500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.252376</td>\n","      <td>0.340453</td>\n","      <td>0.891500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.251203</td>\n","      <td>0.338168</td>\n","      <td>0.892100</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.250024</td>\n","      <td>0.335941</td>\n","      <td>0.893000</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.248844</td>\n","      <td>0.333719</td>\n","      <td>0.893800</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.247661</td>\n","      <td>0.331549</td>\n","      <td>0.894200</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.246501</td>\n","      <td>0.329384</td>\n","      <td>0.895500</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.245357</td>\n","      <td>0.327227</td>\n","      <td>0.896200</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.244216</td>\n","      <td>0.325115</td>\n","      <td>0.896800</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.243080</td>\n","      <td>0.323029</td>\n","      <td>0.897300</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.241962</td>\n","      <td>0.320951</td>\n","      <td>0.897900</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.240842</td>\n","      <td>0.318895</td>\n","      <td>0.898600</td>\n","      <td>00:00</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.239734</td>\n","      <td>0.316873</td>\n","      <td>0.899600</td>\n","      <td>00:00</td>\n","    </tr>\n","  </tbody>\n","</table>"]},"metadata":{}}],"source":["learn.fit(100, lr, reset_opt=True) #after 100 epochs, we end up at ~90% accuracy.\n","\n","#We also see that our in-sample and out-of-sample losses are closing in on each other - a sign that we are not overfitting"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyN2n8CCEmjxBvkEyFTEOWsb"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}